import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
import numpy as np
import pandas as pd
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import linkage
from scipy.cluster.hierarchy import dendrogram
from scipy.cluster.hierarchy import set_link_color_palette
from sklearn.cluster import AgglomerativeClustering

from ml.utils.indexPrinter import indexPrinter
iP = indexPrinter()


# generate some random samples - remember, that real-world data should be standardized, to avoid different 'weights' for different dimensions/features
np.random.seed(123)
variables = ['X', 'Y', 'Z']
labels = ['ID_0', 'ID_1', 'ID_2', 'ID_3', 'ID_4']
X = np.random.random_sample([5, 3])*10
df = pd.DataFrame(X, columns=variables, index=labels)
iP.print(df)  # (1) show generated samples


'''
Grouping clusters in bottom-up fashion - agglomerative (start with n-clusters [n-sample count] and merge the closest ones until 
there is one big cluster); there is also a top-down fashion - divisive (start with single cluster and divide)
'''
# generate distance-matrix using 'pdist' and 'squareform'
pairwiseDistances = pdist(df, metric='euclidean')
iP.print(pairwiseDistances)  # (2) show the pairwiseDistances -> upper-triangular distance matrix (condensed)
row_dist = pd.DataFrame(squareform(pairwiseDistances), columns=labels, index=labels)
iP.print(row_dist)  # (3) show the square distance-matrix

'''
We can either pass a condensed distance matrix (upper triangular: pairwiseDistances) from the `pdist` function, or we can pass the 
"original" data array and define the `metric='euclidean'` argument in `linkage`. However, we should not pass the 
squareform distance matrix, which would yield different distance values although the overall clustering could be the same.

Methods: 
  (1) single (nearest points in neighbouring clusters), 
  (2) complete (farthest points in neighbouring clusters),
  (3) average (obvious),
  (4) centroid (obvious),
  (5) ward (tries to minimize the variance),
  more: https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html
'''
# 1. incorrect approach: Squareform distance matrix
row_clusters = linkage(row_dist, method='complete', metric='euclidean')  # (warning)
badDF = pd.DataFrame(row_clusters,
             columns=['row label 1', 'row label 2', 'distance', 'no. of items in clust.'],
             index=['cluster %d' % (i + 1) for i in range(row_clusters.shape[0])])
iP.print(badDF)  # (4) BAD row_clusters

# 2. correct approach: Condensed distance matrix
row_clusters = linkage(pairwiseDistances, method='complete')
goodDF1 = pd.DataFrame(row_clusters,
             columns=['row label 1', 'row label 2', 'distance', 'no. of items in clust.'],
             index=['cluster %d' % (i + 1) for i in range(row_clusters.shape[0])])
iP.print(goodDF1)  # (5) GOOD (#1) row_clusters

# 3. correct approach: Input sample matrix
row_clusters = linkage(df.values, method='complete', metric='euclidean')
goodDF2 = pd.DataFrame(row_clusters,
             columns=['row label 1', 'row label 2', 'distance', 'no. of items in clust.'],
             index=['cluster %d' % (i + 1) for i in range(row_clusters.shape[0])])
iP.print(goodDF2)  # (6) GOOD (#2) row_clusters

# plot the dendrogram using obtained row_clusters (containing subsequent clusters generated by merging further points)
blackDendrogram = False
plt.figure(num='Dendrogram of hierarchical grouping algorithm')
if (blackDendrogram):
    set_link_color_palette(['black'])
    row_dendr = dendrogram(row_clusters, labels=labels, color_threshold=np.inf)
else: row_dendr = dendrogram(row_clusters, labels=labels)
plt.tight_layout()
plt.ylabel('Euclidean distance')
plt.show()


# Attaching dendrograms to a heat map:
# plot row dendrogram
fig = plt.figure(figsize=(8, 8), facecolor='white', num='Dendrogram of hierarchical grouping algorithm with heatmap (coords. X/Y/Z)')
axd = fig.add_axes([0.09, 0.1, 0.2, 0.6])  # [x-axis position, y-axis position, width, height]
row_dendr = dendrogram(row_clusters, orientation='left')  # rotated by 90 deg
# reorder data with respect to clustering
df_rowclust = df.iloc[row_dendr['leaves'][::-1]]
axd.set_xticks([])
axd.set_yticks([])
# remove axes spines from dendrogram
for i in axd.spines.values():
    i.set_visible(False)
# plot heatmap
axm = fig.add_axes([0.23, 0.1, 0.6, 0.6])  # [x-axis position, y-axis position, width, height]
cax = axm.matshow(df_rowclust, interpolation='nearest', cmap='hot_r')
fig.colorbar(cax)
xticks_loc = axm.get_xticks().tolist()
axm.xaxis.set_major_locator(mticker.FixedLocator(xticks_loc))
axm.set_xticklabels([''] + list(df_rowclust.columns) + [''])
yticks_loc = axm.get_yticks().tolist()
axm.yaxis.set_major_locator(mticker.FixedLocator(yticks_loc))
axm.set_yticklabels([''] + list(df_rowclust.index) + [''])
plt.show()


# Applying agglomerative (i.e., bottom-up) clustering via scikit-learn
ac = AgglomerativeClustering(n_clusters=3,
                             affinity='euclidean',
                             linkage='complete')
labels = ac.fit_predict(X)
iP.print('Cluster labels: %s' % labels)  # (7) 1&2 / 0&4 / 3

ac = AgglomerativeClustering(n_clusters=2,
                             affinity='euclidean',
                             linkage='complete')
labels = ac.fit_predict(X)
iP.print('Cluster labels: %s' % labels)  # (8) 1&2 / 0&4&3  (because 3 is closer to 0&4 cluster than 1&2, what can be seen from dendrogram)